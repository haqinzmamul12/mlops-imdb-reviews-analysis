name: CI Pipeline 
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint-test:
    name: Format, Lint and Test 
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo 
        uses: actions/checkout@v3 

      - name: Install Git LFS
        run: |
          sudo apt-get update
          sudo apt-get install -y git-lfs
          git lfs install
          git lfs pull

      - name: Set up Python
        uses: actions/setup-python@v4 
        with:
          python-version: "3.10"

      - name: Install dependencies 
        run: | 
          pip install -r requirements.txt 
          pip install pytest black flake8 dvc[s3]

      # ---- DVC STEPS ----
      - name: Pull dataset from DVC remote
        run: |
          dvc pull
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Verify dataset is available
        run: ls -lh data/

      # ---- FORMAT + LINT ----
      - name: Reformat code with Black
        run: black .  # reformats all Python files from repo root

      - name: Check formatting with Black
        run: black --check .

      - name: Lint with Flake8
        run: flake8 .

      # ---- TEST ----
      - name: Run unit tests
        run: pytest tests/

  airflow-pipeline:
    name: Airflow DAG Run
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        options: >-
          --health-cmd "pg_isready -U airflow"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      # 1. Checkout code
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Set up Python (for CLI / client side if needed)
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # 3. Install Docker Compose
      - name: Install Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      # 4. Build Airflow Docker image
      - name: Build Airflow image
        run: |
          docker-compose -f docker-compose.yaml build

      # 5. Start Airflow (fix perms → init → run services)
      - name: Start Airflow
        run: |
          # Step A: Run permissions fix
          docker-compose -f docker-compose.yaml up -d airflow-permissions
          sleep 5

          # Step B: Run airflow-init after perms fixed
          docker-compose -f docker-compose.yaml up airflow-init
          sleep 10

          # Step C: Start core services
          docker-compose -f docker-compose.yaml up -d airflow-scheduler airflow-webserver
          sleep 60  # give Airflow enough time to be ready

      # 6. Trigger DAG run
      - name: Trigger ML pipeline DAG
        run: |
          RUN_ID="ci_run_$(date +%s)"
          docker exec airflow-scheduler airflow dags trigger ml_pipeline_dag --run-id "$RUN_ID"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV

      # 7. Monitor DAG run (basic check)
      - name: Monitor DAG run
        run: |
          echo "Checking DAG run status..."
          docker exec airflow-scheduler airflow dags list-runs -d ml_pipeline_dag | head -n 20