name: CI Pipeline 
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint-test:
    name: Format, Lint and Test 
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo 
        uses: actions/checkout@v3 

      - name: Install Git LFS
        run: |
          sudo apt-get install git-lfs
          git lfs install
          git lfs pull

      - name: Set up Python
        uses: actions/setup-python@v4 
        with:
          python-version: "3.10"

      - name: Install dependencies 
        run: | 
          pip install -r requirements.txt 
          pip install pytest black flake8 dvc[s3]

      # # ---- DVC STEPS ----
      # - name: Pull dataset from DVC remote
      #   run: |
      #     dvc pull
      #   env:
      #     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_DEFAULT_REGION: us-east-1

      # - name: Verify dataset is available
      #   run: ls -lh data/

      # ---- FORMAT + LINT ----
      - name: Reformat code with Black
        run: black .  # reformats all Python files from repo root

      - name: Check formatting with Black
        run: black --check .

      # - name: Lint with Flake8
      #   run: flake8 .
      
      # ---- TEST ----
      - name: Run unit tests
        run: pytest tests/

  airflow-pipeline:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        options: >-
          --health-cmd "pg_isready -U airflow"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      # 1. Checkout code
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Set up Python (for CLI / client side if needed)
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # 3. Install Docker Compose
      - name: Install Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      # 4. Build Airflow Docker image
      - name: Build Airflow image
        run: |
          docker-compose -f docker-compose.yaml build

      # 5. Start Airflow (scheduler + webserver + workers)
      - name: Start Airflow
        run: |
          docker-compose -f docker-compose.yaml up -d
          sleep 60  # give time for Airflow to initialize

      # 6. Trigger DAG run
      - name: Trigger ML pipeline DAG
        run: |
          docker exec airflow-scheduler airflow dags trigger -d ml_pipeline_dag

      # 7. Wait for DAG to finish (optional, for CI success/failure)
      - name: Monitor DAG run
        run: |
          docker exec airflow-scheduler airflow dags state ml_pipeline_dag $(date +%Y-%m-%d)T00:00:00+00:00

  
         



