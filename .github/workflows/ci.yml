name: CI Pipeline 
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint-test:
    name: Format, Lint and Test 
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo 
        uses: actions/checkout@v3 

      - name: Install Git LFS
        run: |
          sudo apt-get install git-lfs
          git lfs install
          git lfs pull

      - name: Set up Python
        uses: actions/setup-python@v4 
        with:
          python-version: "3.10"

      - name: Install dependencies 
        run: | 
          pip install -r requirements.txt 
          pip install pytest black flake8 dvc[s3]

      # # ---- DVC STEPS ----
      # - name: Pull dataset from DVC remote
      #   run: |
      #     dvc pull
      #   env:
      #     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_DEFAULT_REGION: us-east-1

      # - name: Verify dataset is available
      #   run: ls -lh data/

      # ---- FORMAT + LINT ----
      - name: Reformat code with Black
        run: black .  # reformats all Python files from repo root

      - name: Check formatting with Black
        run: black --check .

      # - name: Lint with Flake8
      #   run: flake8 .
      
      # ---- TEST ----
      - name: Run unit tests
        run: pytest tests/

  run-ml-pipeline:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repo
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'   # or 3.10 (check your airflow version compatibility)

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install apache-airflow==2.7.2 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.2/constraints-3.9.txt"
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 4. Run ML pipeline DAG script
      - name: Run ML pipeline DAG script
        run: |
          python airflow_orchestrator/dags/ml_pipeline_dag.py

  
         



