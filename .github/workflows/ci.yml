name: CI Pipeline 
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint-test:
    name: Format, Lint and Test 
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo 
        uses: actions/checkout@v3 

      - name: Install Git LFS
        run: |
          sudo apt-get update
          sudo apt-get install -y git-lfs
          git lfs install
          git lfs pull

      - name: Set up Python
        uses: actions/setup-python@v4 
        with:
          python-version: "3.10"

      - name: Install dependencies 
        run: | 
          pip install -r requirements.txt 
          pip install pytest black flake8 dvc[s3]

      # ---- DVC STEPS ----
      - name: Pull dataset from DVC remote
        run: |
          dvc pull
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Verify dataset is available
        run: ls -lh dataset/

      # ---- FORMAT + LINT ----
      - name: Reformat code with Black
        run: black .  # reformats all Python files from repo root

      # - name: Check formatting with Black
      #   run: black --check .

      # - name: Lint with Flake8
      #   run: flake8 .

      # ---- TEST ----
      - name: Run unit tests
        run: pytest tests/

  airflow-pipeline:
    runs-on: ubuntu-latest

    services:
      docker:
        image: docker:20.10-dind
        options: --privileged

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        # with:
        #   lfs: true    # ← This tells GitHub Actions to download LFS files automatically

      # - name: Install Git LFS 
      #   run: |
      #     git lfs install
      #     git lfs pull


      - name: Set up Docker
        uses: docker/setup-buildx-action@v3


      - name: Start Airflow using existing Image
        run: |
          export NGROK_AUTHTOKEN=${{ secrets.NGROK_AUTHTOKEN }}
          docker compose build --no-cache
          docker compose up -d

          echo "Restart Airflow container to ensure DAG + volumes are ready"
          docker restart airflow

          echo "Waiting for Airflow to be ready..."
          sleep 10

          echo "Airflow logs"
          docker logs airflow

          echo "container logs:"
          docker ps -a | grep airflow

          echo "airflow dags list:"
          docker exec airflow airflow dags list

          echo "Checking DAG file inside container:"
          docker exec airflow ls -l /opt/airflow

          # echo "Checking src file inside container:"
          # docker exec airflow ls -l /opt/airflow/src

          echo "Dataset files are:"
          docker exec airflow ls -l /opt/airflow/dataset/
  


      - name: Check DAG import errors
        run: 
          docker exec airflow airflow dags list-import-errors



      - name: Get Airflow ngrok URL
        run: |
          echo "⏳ Waiting for ngrok-airflow to initialize..."
          sleep 10
          docker logs ngrok-mlflow 2>&1 | grep -oE 'https://[a-z0-9]+\.ngrok-free\.app' | head -n 1

      - name: Publish Airflow ngrok URL
        run: |
          AIRFLOW_URL=$(docker logs ngrok-airflow 2>&1 | jq -r 'select(.msg=="started tunnel") | .url' | head -n 1)
          echo "Airflow UI: $AIRFLOW_URL" >> $GITHUB_STEP_SUMMARY


      # - name: Get MLflow ngrok URL
      #   run: |
      #     echo "⏳ Waiting for ngrok-mlflow to initialize..."
      #     sleep 10
      #     docker logs ngrok-mlflow 2>&1 | jq -r 'select(.msg=="started tunnel") | .url' | head -n 1

      # - name: Publish MLflow ngrok URL
      #   run: |
      #     MLFLOW_URL=$(docker logs ngrok-mlflow 2>&1 | jq -r 'select(.msg=="started tunnel") | .url' | head -n 1)
      #     echo "MLflow UI: $MLFLOW_URL" >> $GITHUB_STEP_SUMMARY



      - name: Wait for Airflow
        run: |
          for i in {1..1}; do
            echo "=== Iteration $i ==="
            echo "--> Container status:"
            docker ps -a --filter "name=airflow"

            echo "--> Last 20 lines of logs:"
            docker logs --tail 20 airflow || true

            if docker logs airflow 2>&1 | grep -q "Scheduler started"; then
              echo "✅ Airflow is ready!"
              break
            fi

            echo "⏳ Waiting for Airflow to start..."
            sleep 10
          done



      - name: Trigger DAG
        run: |
          docker exec airflow airflow dags trigger ml_pipeline_dag
          sleep 20
          
          # echo "Testing task: ingest_task"
          # docker exec airflow airflow tasks test ml_pipeline_dag ingest_task $(date +%Y-%m-%d)

          # echo "Testing task: preprocess_task"
          # docker exec airflow airflow tasks test ml_pipeline_dag preprocess_task $(date +%Y-%m-%d)

          # echo "Testing task: train_task"
          # docker exec airflow airflow tasks test ml_pipeline_dag train_task $(date +%Y-%m-%d)

          # echo "Testing task: evaluate_task"
          # docker exec airflow airflow tasks test ml_pipeline_dag evaluate_task $(date +%Y-%m-%d)


      - name: Keep alive for 5 mints
        run: sleep 10