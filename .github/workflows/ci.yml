name: CI Pipeline 
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint-test:
    name: Format, Lint and Test 
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo 
        uses: actions/checkout@v3 

      - name: Install Git LFS
        run: |
          sudo apt-get install git-lfs
          git lfs install
          git lfs pull

      - name: Set up Python
        uses: actions/setup-python@v4 
        with:
          python-version: "3.10"

      - name: Install dependencies 
        run: | 
          pip install -r requirements.txt 
          pip install pytest black flake8 dvc[s3]

      # # ---- DVC STEPS ----
      # - name: Pull dataset from DVC remote
      #   run: |
      #     dvc pull
      #   env:
      #     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_DEFAULT_REGION: us-east-1

      # - name: Verify dataset is available
      #   run: ls -lh data/

      # ---- FORMAT + LINT ----
      - name: Reformat code with Black
        run: black .  # reformats all Python files from repo root

      - name: Check formatting with Black
        run: black --check .

      # - name: Lint with Flake8
      #   run: flake8 .
      
      # ---- TEST ----
      - name: Run unit tests
        run: pytest tests/

  airflow-dag-check:
    name: Run ML Pipelines
    runs-on: ubuntu-latest 

    steps:
      - name: checkout code 
        uses: actions/checkout@v3 

      - name: set up python 
        uses: actions/setup-python@v4 
        with:
          python-version: '3.9'

      - name: Install Airflow and dependencies 
        run: | 
          export AIRFLOW_VERSION=2.8.1 
          export PYTHON_VERSION=3.9 
          export CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/${PYTHON_VERSION}.txt"

          python -m pip install --upgrade pip 
          pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
          pip install -r requirements.txt

      - name: Set PYTHONPATH
        run: | 
          echo "PYTHONPATH=$(pwd)" >> $GITHUB_ENV

      - name: Initialize Airflow 
        run: | 
          airflow db init 

      - name: Copy DAGs 
        run: | 
          mkdir -p ~/airflow/dags 
          cp -r airflow_orchestrator/dags/* ~/airflow/dags/ 

      - name: List available DAGs 
        run: | 
          airfllow dags list 

      - name: Trigger ML pipeline DAG 
        run: | 
          echo "Triggering DAG: ml_pipeline_dag"
          airflow dags trigger -e 2025-08-20 ml_pipline_dag 

      - name: Wait and monitor DAG run 
        run: | 
          echo "Waiting 60s for DAG to execute tasks..."
          sleep 60 
          echo "Checking DAG run status:" 
          airflow dags list-runs -d ml_pipeline_dag 

      - name: Show task logs 
        run: | 
          echo "Fetching logs from the most recent DSG run:"
          DAG_RUN_ID =$(airflow dags list-runs -d ml_pipeline_dag -o json | jq -r '.[0].run_id')
          echo "DAG RUN ID: $DAG_RUN_ID"
          echo "Listing all tasks in DAG 'ml_pipeline_dag':"
          airflow tasks list ml_pipeline_dag 
          
          echo "Printing logs for each task in the DAG..." 
          for TASK_ID in data_ingestion data_preprocessing train_model evaluate_models 
          do
            echo "=============================================="
            echo "Logs for Task: $TASK_ID"
            echo "=============================================="
            airflow tasks logs ml_pipeline_dag "$TASK_ID" "$DAG_RUN_ID" || echo "No logs for $TASK_ID"
            ech ""
          done


